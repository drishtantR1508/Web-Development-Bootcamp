{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package tensorflow_datasets:\n",
      "\n",
      "NAME\n",
      "    tensorflow_datasets - `tensorflow_datasets` (`tfds`) defines a collection of datasets ready-to-use with TensorFlow.\n",
      "\n",
      "DESCRIPTION\n",
      "    Each dataset is defined as a `tfds.core.DatasetBuilder`, which encapsulates\n",
      "    the logic to download the dataset and construct an input pipeline, as well as\n",
      "    contains the dataset documentation (version, splits, number of examples, etc.).\n",
      "    \n",
      "    The main library entrypoints are:\n",
      "    \n",
      "    * `tfds.builder`: fetch a `tfds.core.DatasetBuilder` by name\n",
      "    * `tfds.load`: convenience method to construct a builder, download the data, and\n",
      "      create an input pipeline, returning a `tf.data.Dataset`.\n",
      "    \n",
      "    Documentation:\n",
      "    \n",
      "    * These API docs\n",
      "    * [Available datasets](https://www.tensorflow.org/datasets/catalog/overview)\n",
      "    * [Colab tutorial](https://colab.research.google.com/github/tensorflow/datasets/blob/master/docs/overview.ipynb)\n",
      "    * [Add a dataset](https://www.tensorflow.org/datasets/add_dataset)\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    audio (package)\n",
      "    core (package)\n",
      "    image (package)\n",
      "    image_classification (package)\n",
      "    import_test\n",
      "    import_testing_test\n",
      "    object_detection (package)\n",
      "    proto (package)\n",
      "    public_api\n",
      "    question_answering (package)\n",
      "    scripts (package)\n",
      "    structured (package)\n",
      "    summarization (package)\n",
      "    testing (package)\n",
      "    text (package)\n",
      "    translate (package)\n",
      "    version\n",
      "    video (package)\n",
      "\n",
      "SUBMODULES\n",
      "    decode\n",
      "    download\n",
      "    features\n",
      "    tf_compat\n",
      "    units\n",
      "    visualization\n",
      "\n",
      "CLASSES\n",
      "    builtins.str(builtins.object)\n",
      "        tensorflow_datasets.core.splits.Split\n",
      "    enum.Enum(builtins.object)\n",
      "        tensorflow_datasets.core.download.util.GenerateMode\n",
      "    tensorflow_datasets.core.dataset_builder.DatasetBuilder(builtins.object)\n",
      "        tensorflow_datasets.core.custom_dataset.image_folder.ImageFolder\n",
      "        tensorflow_datasets.core.custom_dataset.translate_folder.TranslateFolder\n",
      "    tensorflow_datasets.core.utils.read_config._ReadConfig(builtins.object)\n",
      "        tensorflow_datasets.core.utils.read_config.ReadConfig\n",
      "    \n",
      "    class GenerateMode(enum.Enum)\n",
      "     |  GenerateMode(value, names=None, *, module=None, qualname=None, type=None, start=1)\n",
      "     |  \n",
      "     |  `Enum` for how to treat pre-existing downloads and data.\n",
      "     |  \n",
      "     |  The default mode is `REUSE_DATASET_IF_EXISTS`, which will reuse both\n",
      "     |  raw downloads and the prepared dataset if they exist.\n",
      "     |  \n",
      "     |  The generations modes:\n",
      "     |  \n",
      "     |  |                                    | Downloads | Dataset |\n",
      "     |  | -----------------------------------|-----------|---------|\n",
      "     |  | `REUSE_DATASET_IF_EXISTS` (default)| Reuse     | Reuse   |\n",
      "     |  | `REUSE_CACHE_IF_EXISTS`            | Reuse     | Fresh   |\n",
      "     |  | `FORCE_REDOWNLOAD`                 | Fresh     | Fresh   |\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GenerateMode\n",
      "     |      enum.Enum\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  FORCE_REDOWNLOAD = <GenerateMode.FORCE_REDOWNLOAD: 'force_redownload'>\n",
      "     |  \n",
      "     |  REUSE_CACHE_IF_EXISTS = <GenerateMode.REUSE_CACHE_IF_EXISTS: 'reuse_ca...\n",
      "     |  \n",
      "     |  REUSE_DATASET_IF_EXISTS = <GenerateMode.REUSE_DATASET_IF_EXISTS: 'reus...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from enum.Enum:\n",
      "     |  \n",
      "     |  name\n",
      "     |      The name of the Enum member.\n",
      "     |  \n",
      "     |  value\n",
      "     |      The value of the Enum member.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from enum.EnumMeta:\n",
      "     |  \n",
      "     |  __members__\n",
      "     |      Returns a mapping of member name->value.\n",
      "     |      \n",
      "     |      This mapping lists all enum members, including aliases. Note that this\n",
      "     |      is a read-only view of the internal mapping.\n",
      "    \n",
      "    class ImageFolder(tensorflow_datasets.core.dataset_builder.DatasetBuilder)\n",
      "     |  ImageFolder(root_dir: str)\n",
      "     |  \n",
      "     |  Generic image classification dataset created from manual directory.\n",
      "     |  \n",
      "     |  `ImageFolder` creates a `tf.data.Dataset` reading the original image files.\n",
      "     |  \n",
      "     |  The data directory should have the following structure:\n",
      "     |  \n",
      "     |  ```\n",
      "     |  path/to/image_dir/\n",
      "     |    split_name/  # Ex: 'train'\n",
      "     |      label1/  # Ex: 'airplane' or '0015'\n",
      "     |        xxx.png\n",
      "     |        xxy.png\n",
      "     |        xxz.png\n",
      "     |      label2/\n",
      "     |        xxx.png\n",
      "     |        xxy.png\n",
      "     |        xxz.png\n",
      "     |    split_name/  # Ex: 'test'\n",
      "     |      ...\n",
      "     |  ```\n",
      "     |  \n",
      "     |  To use it:\n",
      "     |  \n",
      "     |  ```\n",
      "     |  builder = tfds.ImageFolder('path/to/image_dir/')\n",
      "     |  print(builder.info)  # num examples, labels... are automatically calculated\n",
      "     |  ds = builder.as_dataset(split='train', shuffle_files=True)\n",
      "     |  tfds.show_examples(ds, builder.info)\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ImageFolder\n",
      "     |      tensorflow_datasets.core.dataset_builder.DatasetBuilder\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root_dir: str)\n",
      "     |      Constructs a DatasetBuilder.\n",
      "     |      \n",
      "     |      Callers must pass arguments as keyword arguments.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        data_dir: `str`, directory to read/write data. Defaults to the value of\n",
      "     |          the environment variable TFDS_DATA_DIR, if set, otherwise falls back to\n",
      "     |          \"~/tensorflow_datasets\".\n",
      "     |        config: `tfds.core.BuilderConfig` or `str` name, optional configuration\n",
      "     |          for the dataset that affects the data generated on disk. Different\n",
      "     |          `builder_config`s will have their own subdirectories and versions.\n",
      "     |        version: `str`. Optional version at which to load the dataset. An error is\n",
      "     |          raised if specified version cannot be satisfied. Eg: '1.2.3', '1.2.*'.\n",
      "     |          The special value \"experimental_latest\" will use the highest version,\n",
      "     |          even if not default. This is not recommended unless you know what you\n",
      "     |          are doing, as the version could be broken.\n",
      "     |  \n",
      "     |  download_and_prepare(self, **kwargs)\n",
      "     |      Downloads and prepares dataset for reading.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        download_dir: `str`, directory where downloaded files are stored.\n",
      "     |          Defaults to \"~/tensorflow-datasets/downloads\".\n",
      "     |        download_config: `tfds.download.DownloadConfig`, further configuration for\n",
      "     |          downloading and preparing dataset.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        IOError: if there is not enough disk space available.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  VERSION = <tensorflow_datasets.core.utils.version.Version object>\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  name = 'image_folder'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from tensorflow_datasets.core.dataset_builder.DatasetBuilder:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  as_dataset(self, split=None, batch_size=None, shuffle_files=False, decoders=None, read_config=None, as_supervised=False)\n",
      "     |      Constructs a `tf.data.Dataset`.\n",
      "     |      \n",
      "     |      Callers must pass arguments as keyword arguments.\n",
      "     |      \n",
      "     |      The output types vary depending on the parameters. Examples:\n",
      "     |      \n",
      "     |      ```python\n",
      "     |      builder = tfds.builder('imdb_reviews')\n",
      "     |      builder.download_and_prepare()\n",
      "     |      \n",
      "     |      # Default parameters: Returns the dict of tf.data.Dataset\n",
      "     |      ds_all_dict = builder.as_dataset()\n",
      "     |      assert isinstance(ds_all_dict, dict)\n",
      "     |      print(ds_all_dict.keys())  # ==> ['test', 'train', 'unsupervised']\n",
      "     |      \n",
      "     |      assert isinstance(ds_all_dict['test'], tf.data.Dataset)\n",
      "     |      # Each dataset (test, train, unsup.) consists of dictionaries\n",
      "     |      # {'label': <tf.Tensor: .. dtype=int64, numpy=1>,\n",
      "     |      #  'text': <tf.Tensor: .. dtype=string, numpy=b\"I've watched the movie ..\">}\n",
      "     |      # {'label': <tf.Tensor: .. dtype=int64, numpy=1>,\n",
      "     |      #  'text': <tf.Tensor: .. dtype=string, numpy=b'If you love Japanese ..'>}\n",
      "     |      \n",
      "     |      # With as_supervised: tf.data.Dataset only contains (feature, label) tuples\n",
      "     |      ds_all_supervised = builder.as_dataset(as_supervised=True)\n",
      "     |      assert isinstance(ds_all_supervised, dict)\n",
      "     |      print(ds_all_supervised.keys())  # ==> ['test', 'train', 'unsupervised']\n",
      "     |      \n",
      "     |      assert isinstance(ds_all_supervised['test'], tf.data.Dataset)\n",
      "     |      # Each dataset (test, train, unsup.) consists of tuples (text, label)\n",
      "     |      # (<tf.Tensor: ... dtype=string, numpy=b\"I've watched the movie ..\">,\n",
      "     |      #  <tf.Tensor: ... dtype=int64, numpy=1>)\n",
      "     |      # (<tf.Tensor: ... dtype=string, numpy=b\"If you love Japanese ..\">,\n",
      "     |      #  <tf.Tensor: ... dtype=int64, numpy=1>)\n",
      "     |      \n",
      "     |      # Same as above plus requesting a particular split\n",
      "     |      ds_test_supervised = builder.as_dataset(as_supervised=True, split='test')\n",
      "     |      assert isinstance(ds_test_supervised, tf.data.Dataset)\n",
      "     |      # The dataset consists of tuples (text, label)\n",
      "     |      # (<tf.Tensor: ... dtype=string, numpy=b\"I've watched the movie ..\">,\n",
      "     |      #  <tf.Tensor: ... dtype=int64, numpy=1>)\n",
      "     |      # (<tf.Tensor: ... dtype=string, numpy=b\"If you love Japanese ..\">,\n",
      "     |      #  <tf.Tensor: ... dtype=int64, numpy=1>)\n",
      "     |      ```\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        split: Which split of the data to load (e.g. `'train'`, `'test'`\n",
      "     |          `['train', 'test']`, `'train[80%:]'`,...). See our\n",
      "     |          [split API guide](https://www.tensorflow.org/datasets/splits).\n",
      "     |          If `None`, will return all splits in a `Dict[Split, tf.data.Dataset]`.\n",
      "     |        batch_size: `int`, batch size. Note that variable-length features will\n",
      "     |          be 0-padded if `batch_size` is set. Users that want more custom behavior\n",
      "     |          should use `batch_size=None` and use the `tf.data` API to construct a\n",
      "     |          custom pipeline. If `batch_size == -1`, will return feature\n",
      "     |          dictionaries of the whole dataset with `tf.Tensor`s instead of a\n",
      "     |          `tf.data.Dataset`.\n",
      "     |        shuffle_files: `bool`, whether to shuffle the input files. Defaults to\n",
      "     |          `False`.\n",
      "     |        decoders: Nested dict of `Decoder` objects which allow to customize the\n",
      "     |          decoding. The structure should match the feature structure, but only\n",
      "     |          customized feature keys need to be present. See\n",
      "     |          [the guide](https://github.com/tensorflow/datasets/tree/master/docs/decode.md)\n",
      "     |          for more info.\n",
      "     |        read_config: `tfds.ReadConfig`, Additional options to configure the\n",
      "     |          input pipeline (e.g. seed, num parallel reads,...).\n",
      "     |        as_supervised: `bool`, if `True`, the returned `tf.data.Dataset`\n",
      "     |          will have a 2-tuple structure `(input, label)` according to\n",
      "     |          `builder.info.supervised_keys`. If `False`, the default,\n",
      "     |          the returned `tf.data.Dataset` will have a dictionary with all the\n",
      "     |          features.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        `tf.data.Dataset`, or if `split=None`, `dict<key: tfds.Split, value:\n",
      "     |        tfds.data.Dataset>`.\n",
      "     |      \n",
      "     |        If `batch_size` is -1, will return feature dictionaries containing\n",
      "     |        the entire dataset in `tf.Tensor`s instead of a `tf.data.Dataset`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from tensorflow_datasets.core.dataset_builder.DatasetBuilder:\n",
      "     |  \n",
      "     |  builder_config\n",
      "     |      `tfds.core.BuilderConfig` for this builder.\n",
      "     |  \n",
      "     |  builder_configs\n",
      "     |      classmethod(function) -> method\n",
      "     |      \n",
      "     |      Convert a function to be a class method.\n",
      "     |      \n",
      "     |      A class method receives the class as implicit first argument,\n",
      "     |      just like an instance method receives the instance.\n",
      "     |      To declare a class method, use this idiom:\n",
      "     |      \n",
      "     |        class C:\n",
      "     |            @classmethod\n",
      "     |            def f(cls, arg1, arg2, ...):\n",
      "     |                ...\n",
      "     |      \n",
      "     |      It can be called either on the class (e.g. C.f()) or on an instance\n",
      "     |      (e.g. C().f()).  The instance is ignored except for its class.\n",
      "     |      If a class method is called for a derived class, the derived class\n",
      "     |      object is passed as the implied first argument.\n",
      "     |      \n",
      "     |      Class methods are different than C++ or Java static methods.\n",
      "     |      If you want those, see the staticmethod builtin.\n",
      "     |  \n",
      "     |  canonical_version\n",
      "     |  \n",
      "     |  data_dir\n",
      "     |  \n",
      "     |  info\n",
      "     |      `tfds.core.DatasetInfo` for this builder.\n",
      "     |  \n",
      "     |  supported_versions\n",
      "     |  \n",
      "     |  version\n",
      "     |  \n",
      "     |  versions\n",
      "     |      Versions (canonical + availables), in preference order.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from tensorflow_datasets.core.dataset_builder.DatasetBuilder:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from tensorflow_datasets.core.dataset_builder.DatasetBuilder:\n",
      "     |  \n",
      "     |  BUILDER_CONFIGS = []\n",
      "     |  \n",
      "     |  IN_DEVELOPMENT = False\n",
      "     |  \n",
      "     |  MANUAL_DOWNLOAD_INSTRUCTIONS = None\n",
      "     |  \n",
      "     |  SUPPORTED_VERSIONS = []\n",
      "    \n",
      "    class ReadConfig(_ReadConfig)\n",
      "     |  ReadConfig(**kwargs)\n",
      "     |  \n",
      "     |  Configures input reading pipeline.\n",
      "     |  \n",
      "     |  Attributes:\n",
      "     |    options: `tf.data.Options()`, dataset options. Those options are added to\n",
      "     |      the default values defined in `tfrecord_reader.py`.\n",
      "     |      Note that when `shuffle_files` is True and no seed is defined,\n",
      "     |      experimental_deterministic will be set to False internally,\n",
      "     |      unless it is defined here.\n",
      "     |    try_autocache: If True (default) and the dataset satisfy the right\n",
      "     |      conditions (dataset small enough, files not shuffled,...) the dataset\n",
      "     |      will be cached during the first iteration (through `ds = ds.cache()`).\n",
      "     |    shuffle_seed: `tf.int64`, seeds forwarded to `tf.data.Dataset.shuffle` when\n",
      "     |      `shuffle_files=True`.\n",
      "     |    shuffle_reshuffle_each_iteration: `bool`, forwarded to\n",
      "     |      `tf.data.Dataset.shuffle` when `shuffle_files=True`.\n",
      "     |    interleave_cycle_length: `int`, forwarded to `tf.data.Dataset.interleave`.\n",
      "     |      Default to 16.\n",
      "     |    interleave_block_length: `int`, forwarded to `tf.data.Dataset.interleave`.\n",
      "     |      Default to 16.\n",
      "     |    input_context: `tf.distribute.InputContext`, if set, each worker\n",
      "     |      will read a different set of file. For more info, see the\n",
      "     |      [distribute_datasets_from_function\n",
      "     |      documentation](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#experimental_distribute_datasets_from_function).\n",
      "     |      Note:\n",
      "     |  \n",
      "     |      * Each workers will always read the same subset of files. `shuffle_files`\n",
      "     |        only shuffle files within each worker.\n",
      "     |      * If `info.splits[split].num_shards < input_context.num_input_pipelines`,\n",
      "     |        an error will be raised, as some workers would be empty.\n",
      "     |  \n",
      "     |    experimental_interleave_sort_fn: Function with signature\n",
      "     |      `List[FileDict] -> List[FileDict]`, which takes the list of\n",
      "     |      `dict(file: str, take: int, skip: int)` and returns the modified version\n",
      "     |      to read. This can be used to sort/shuffle the shards to read in\n",
      "     |      a custom order, instead of relying on `shuffle_files=True`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ReadConfig\n",
      "     |      _ReadConfig\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _ReadConfig:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, other)\n",
      "     |      Automatically created by attrs.\n",
      "     |  \n",
      "     |  __gt__(self, other)\n",
      "     |      Automatically created by attrs.\n",
      "     |  \n",
      "     |  __le__(self, other)\n",
      "     |      Automatically created by attrs.\n",
      "     |  \n",
      "     |  __lt__(self, other)\n",
      "     |      Automatically created by attrs.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Check equality and either forward a NotImplemented or return the result\n",
      "     |      negated.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Automatically created by attrs.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _ReadConfig:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  interleave_parallel_reads\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from _ReadConfig:\n",
      "     |  \n",
      "     |  __attrs_attrs__ = (Attribute(name='options', default=Factory(facto...y...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class Split(builtins.str)\n",
      "     |  `Enum` for dataset splits.\n",
      "     |  \n",
      "     |  Datasets are typically split into different subsets to be used at various\n",
      "     |  stages of training and evaluation.\n",
      "     |  \n",
      "     |  * `TRAIN`: the training data.\n",
      "     |  * `VALIDATION`: the validation data. If present, this is typically used as\n",
      "     |    evaluation data while iterating on a model (e.g. changing hyperparameters,\n",
      "     |    model architecture, etc.).\n",
      "     |  * `TEST`: the testing data. This is the data to report metrics on. Typically\n",
      "     |    you do not want to use this during model iteration as you may overfit to it.\n",
      "     |  \n",
      "     |  See the\n",
      "     |  [guide on splits](https://github.com/tensorflow/datasets/tree/master/docs/splits.md)\n",
      "     |  for more information.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Split\n",
      "     |      builtins.str\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  TEST = Split('test')\n",
      "     |  \n",
      "     |  TRAIN = Split('train')\n",
      "     |  \n",
      "     |  VALIDATION = Split('validation')\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.str:\n",
      "     |  \n",
      "     |  __add__(self, value, /)\n",
      "     |      Return self+value.\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      Return key in self.\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __format__(self, format_spec, /)\n",
      "     |      Return a formatted version of the string as described by format_spec.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getitem__(self, key, /)\n",
      "     |      Return self[key].\n",
      "     |  \n",
      "     |  __getnewargs__(...)\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __hash__(self, /)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __mod__(self, value, /)\n",
      "     |      Return self%value.\n",
      "     |  \n",
      "     |  __mul__(self, value, /)\n",
      "     |      Return self*value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __rmod__(self, value, /)\n",
      "     |      Return value%self.\n",
      "     |  \n",
      "     |  __rmul__(self, value, /)\n",
      "     |      Return value*self.\n",
      "     |  \n",
      "     |  __sizeof__(self, /)\n",
      "     |      Return the size of the string in memory, in bytes.\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  capitalize(self, /)\n",
      "     |      Return a capitalized version of the string.\n",
      "     |      \n",
      "     |      More specifically, make the first character have upper case and the rest lower\n",
      "     |      case.\n",
      "     |  \n",
      "     |  casefold(self, /)\n",
      "     |      Return a version of the string suitable for caseless comparisons.\n",
      "     |  \n",
      "     |  center(self, width, fillchar=' ', /)\n",
      "     |      Return a centered string of length width.\n",
      "     |      \n",
      "     |      Padding is done using the specified fill character (default is a space).\n",
      "     |  \n",
      "     |  count(...)\n",
      "     |      S.count(sub[, start[, end]]) -> int\n",
      "     |      \n",
      "     |      Return the number of non-overlapping occurrences of substring sub in\n",
      "     |      string S[start:end].  Optional arguments start and end are\n",
      "     |      interpreted as in slice notation.\n",
      "     |  \n",
      "     |  encode(self, /, encoding='utf-8', errors='strict')\n",
      "     |      Encode the string using the codec registered for encoding.\n",
      "     |      \n",
      "     |      encoding\n",
      "     |        The encoding in which to encode the string.\n",
      "     |      errors\n",
      "     |        The error handling scheme to use for encoding errors.\n",
      "     |        The default is 'strict' meaning that encoding errors raise a\n",
      "     |        UnicodeEncodeError.  Other possible values are 'ignore', 'replace' and\n",
      "     |        'xmlcharrefreplace' as well as any other name registered with\n",
      "     |        codecs.register_error that can handle UnicodeEncodeErrors.\n",
      "     |  \n",
      "     |  endswith(...)\n",
      "     |      S.endswith(suffix[, start[, end]]) -> bool\n",
      "     |      \n",
      "     |      Return True if S ends with the specified suffix, False otherwise.\n",
      "     |      With optional start, test S beginning at that position.\n",
      "     |      With optional end, stop comparing S at that position.\n",
      "     |      suffix can also be a tuple of strings to try.\n",
      "     |  \n",
      "     |  expandtabs(self, /, tabsize=8)\n",
      "     |      Return a copy where all tab characters are expanded using spaces.\n",
      "     |      \n",
      "     |      If tabsize is not given, a tab size of 8 characters is assumed.\n",
      "     |  \n",
      "     |  find(...)\n",
      "     |      S.find(sub[, start[, end]]) -> int\n",
      "     |      \n",
      "     |      Return the lowest index in S where substring sub is found,\n",
      "     |      such that sub is contained within S[start:end].  Optional\n",
      "     |      arguments start and end are interpreted as in slice notation.\n",
      "     |      \n",
      "     |      Return -1 on failure.\n",
      "     |  \n",
      "     |  format(...)\n",
      "     |      S.format(*args, **kwargs) -> str\n",
      "     |      \n",
      "     |      Return a formatted version of S, using substitutions from args and kwargs.\n",
      "     |      The substitutions are identified by braces ('{' and '}').\n",
      "     |  \n",
      "     |  format_map(...)\n",
      "     |      S.format_map(mapping) -> str\n",
      "     |      \n",
      "     |      Return a formatted version of S, using substitutions from mapping.\n",
      "     |      The substitutions are identified by braces ('{' and '}').\n",
      "     |  \n",
      "     |  index(...)\n",
      "     |      S.index(sub[, start[, end]]) -> int\n",
      "     |      \n",
      "     |      Return the lowest index in S where substring sub is found,\n",
      "     |      such that sub is contained within S[start:end].  Optional\n",
      "     |      arguments start and end are interpreted as in slice notation.\n",
      "     |      \n",
      "     |      Raises ValueError when the substring is not found.\n",
      "     |  \n",
      "     |  isalnum(self, /)\n",
      "     |      Return True if the string is an alpha-numeric string, False otherwise.\n",
      "     |      \n",
      "     |      A string is alpha-numeric if all characters in the string are alpha-numeric and\n",
      "     |      there is at least one character in the string.\n",
      "     |  \n",
      "     |  isalpha(self, /)\n",
      "     |      Return True if the string is an alphabetic string, False otherwise.\n",
      "     |      \n",
      "     |      A string is alphabetic if all characters in the string are alphabetic and there\n",
      "     |      is at least one character in the string.\n",
      "     |  \n",
      "     |  isascii(self, /)\n",
      "     |      Return True if all characters in the string are ASCII, False otherwise.\n",
      "     |      \n",
      "     |      ASCII characters have code points in the range U+0000-U+007F.\n",
      "     |      Empty string is ASCII too.\n",
      "     |  \n",
      "     |  isdecimal(self, /)\n",
      "     |      Return True if the string is a decimal string, False otherwise.\n",
      "     |      \n",
      "     |      A string is a decimal string if all characters in the string are decimal and\n",
      "     |      there is at least one character in the string.\n",
      "     |  \n",
      "     |  isdigit(self, /)\n",
      "     |      Return True if the string is a digit string, False otherwise.\n",
      "     |      \n",
      "     |      A string is a digit string if all characters in the string are digits and there\n",
      "     |      is at least one character in the string.\n",
      "     |  \n",
      "     |  isidentifier(self, /)\n",
      "     |      Return True if the string is a valid Python identifier, False otherwise.\n",
      "     |      \n",
      "     |      Call keyword.iskeyword(s) to test whether string s is a reserved identifier,\n",
      "     |      such as \"def\" or \"class\".\n",
      "     |  \n",
      "     |  islower(self, /)\n",
      "     |      Return True if the string is a lowercase string, False otherwise.\n",
      "     |      \n",
      "     |      A string is lowercase if all cased characters in the string are lowercase and\n",
      "     |      there is at least one cased character in the string.\n",
      "     |  \n",
      "     |  isnumeric(self, /)\n",
      "     |      Return True if the string is a numeric string, False otherwise.\n",
      "     |      \n",
      "     |      A string is numeric if all characters in the string are numeric and there is at\n",
      "     |      least one character in the string.\n",
      "     |  \n",
      "     |  isprintable(self, /)\n",
      "     |      Return True if the string is printable, False otherwise.\n",
      "     |      \n",
      "     |      A string is printable if all of its characters are considered printable in\n",
      "     |      repr() or if it is empty.\n",
      "     |  \n",
      "     |  isspace(self, /)\n",
      "     |      Return True if the string is a whitespace string, False otherwise.\n",
      "     |      \n",
      "     |      A string is whitespace if all characters in the string are whitespace and there\n",
      "     |      is at least one character in the string.\n",
      "     |  \n",
      "     |  istitle(self, /)\n",
      "     |      Return True if the string is a title-cased string, False otherwise.\n",
      "     |      \n",
      "     |      In a title-cased string, upper- and title-case characters may only\n",
      "     |      follow uncased characters and lowercase characters only cased ones.\n",
      "     |  \n",
      "     |  isupper(self, /)\n",
      "     |      Return True if the string is an uppercase string, False otherwise.\n",
      "     |      \n",
      "     |      A string is uppercase if all cased characters in the string are uppercase and\n",
      "     |      there is at least one cased character in the string.\n",
      "     |  \n",
      "     |  join(self, iterable, /)\n",
      "     |      Concatenate any number of strings.\n",
      "     |      \n",
      "     |      The string whose method is called is inserted in between each given string.\n",
      "     |      The result is returned as a new string.\n",
      "     |      \n",
      "     |      Example: '.'.join(['ab', 'pq', 'rs']) -> 'ab.pq.rs'\n",
      "     |  \n",
      "     |  ljust(self, width, fillchar=' ', /)\n",
      "     |      Return a left-justified string of length width.\n",
      "     |      \n",
      "     |      Padding is done using the specified fill character (default is a space).\n",
      "     |  \n",
      "     |  lower(self, /)\n",
      "     |      Return a copy of the string converted to lowercase.\n",
      "     |  \n",
      "     |  lstrip(self, chars=None, /)\n",
      "     |      Return a copy of the string with leading whitespace removed.\n",
      "     |      \n",
      "     |      If chars is given and not None, remove characters in chars instead.\n",
      "     |  \n",
      "     |  partition(self, sep, /)\n",
      "     |      Partition the string into three parts using the given separator.\n",
      "     |      \n",
      "     |      This will search for the separator in the string.  If the separator is found,\n",
      "     |      returns a 3-tuple containing the part before the separator, the separator\n",
      "     |      itself, and the part after it.\n",
      "     |      \n",
      "     |      If the separator is not found, returns a 3-tuple containing the original string\n",
      "     |      and two empty strings.\n",
      "     |  \n",
      "     |  replace(self, old, new, count=-1, /)\n",
      "     |      Return a copy with all occurrences of substring old replaced by new.\n",
      "     |      \n",
      "     |        count\n",
      "     |          Maximum number of occurrences to replace.\n",
      "     |          -1 (the default value) means replace all occurrences.\n",
      "     |      \n",
      "     |      If the optional argument count is given, only the first count occurrences are\n",
      "     |      replaced.\n",
      "     |  \n",
      "     |  rfind(...)\n",
      "     |      S.rfind(sub[, start[, end]]) -> int\n",
      "     |      \n",
      "     |      Return the highest index in S where substring sub is found,\n",
      "     |      such that sub is contained within S[start:end].  Optional\n",
      "     |      arguments start and end are interpreted as in slice notation.\n",
      "     |      \n",
      "     |      Return -1 on failure.\n",
      "     |  \n",
      "     |  rindex(...)\n",
      "     |      S.rindex(sub[, start[, end]]) -> int\n",
      "     |      \n",
      "     |      Return the highest index in S where substring sub is found,\n",
      "     |      such that sub is contained within S[start:end].  Optional\n",
      "     |      arguments start and end are interpreted as in slice notation.\n",
      "     |      \n",
      "     |      Raises ValueError when the substring is not found.\n",
      "     |  \n",
      "     |  rjust(self, width, fillchar=' ', /)\n",
      "     |      Return a right-justified string of length width.\n",
      "     |      \n",
      "     |      Padding is done using the specified fill character (default is a space).\n",
      "     |  \n",
      "     |  rpartition(self, sep, /)\n",
      "     |      Partition the string into three parts using the given separator.\n",
      "     |      \n",
      "     |      This will search for the separator in the string, starting at the end. If\n",
      "     |      the separator is found, returns a 3-tuple containing the part before the\n",
      "     |      separator, the separator itself, and the part after it.\n",
      "     |      \n",
      "     |      If the separator is not found, returns a 3-tuple containing two empty strings\n",
      "     |      and the original string.\n",
      "     |  \n",
      "     |  rsplit(self, /, sep=None, maxsplit=-1)\n",
      "     |      Return a list of the words in the string, using sep as the delimiter string.\n",
      "     |      \n",
      "     |        sep\n",
      "     |          The delimiter according which to split the string.\n",
      "     |          None (the default value) means split according to any whitespace,\n",
      "     |          and discard empty strings from the result.\n",
      "     |        maxsplit\n",
      "     |          Maximum number of splits to do.\n",
      "     |          -1 (the default value) means no limit.\n",
      "     |      \n",
      "     |      Splits are done starting at the end of the string and working to the front.\n",
      "     |  \n",
      "     |  rstrip(self, chars=None, /)\n",
      "     |      Return a copy of the string with trailing whitespace removed.\n",
      "     |      \n",
      "     |      If chars is given and not None, remove characters in chars instead.\n",
      "     |  \n",
      "     |  split(self, /, sep=None, maxsplit=-1)\n",
      "     |      Return a list of the words in the string, using sep as the delimiter string.\n",
      "     |      \n",
      "     |      sep\n",
      "     |        The delimiter according which to split the string.\n",
      "     |        None (the default value) means split according to any whitespace,\n",
      "     |        and discard empty strings from the result.\n",
      "     |      maxsplit\n",
      "     |        Maximum number of splits to do.\n",
      "     |        -1 (the default value) means no limit.\n",
      "     |  \n",
      "     |  splitlines(self, /, keepends=False)\n",
      "     |      Return a list of the lines in the string, breaking at line boundaries.\n",
      "     |      \n",
      "     |      Line breaks are not included in the resulting list unless keepends is given and\n",
      "     |      true.\n",
      "     |  \n",
      "     |  startswith(...)\n",
      "     |      S.startswith(prefix[, start[, end]]) -> bool\n",
      "     |      \n",
      "     |      Return True if S starts with the specified prefix, False otherwise.\n",
      "     |      With optional start, test S beginning at that position.\n",
      "     |      With optional end, stop comparing S at that position.\n",
      "     |      prefix can also be a tuple of strings to try.\n",
      "     |  \n",
      "     |  strip(self, chars=None, /)\n",
      "     |      Return a copy of the string with leading and trailing whitespace removed.\n",
      "     |      \n",
      "     |      If chars is given and not None, remove characters in chars instead.\n",
      "     |  \n",
      "     |  swapcase(self, /)\n",
      "     |      Convert uppercase characters to lowercase and lowercase characters to uppercase.\n",
      "     |  \n",
      "     |  title(self, /)\n",
      "     |      Return a version of the string where each word is titlecased.\n",
      "     |      \n",
      "     |      More specifically, words start with uppercased characters and all remaining\n",
      "     |      cased characters have lower case.\n",
      "     |  \n",
      "     |  translate(self, table, /)\n",
      "     |      Replace each character in the string using the given translation table.\n",
      "     |      \n",
      "     |        table\n",
      "     |          Translation table, which must be a mapping of Unicode ordinals to\n",
      "     |          Unicode ordinals, strings, or None.\n",
      "     |      \n",
      "     |      The table must implement lookup/indexing via __getitem__, for instance a\n",
      "     |      dictionary or list.  If this operation raises LookupError, the character is\n",
      "     |      left untouched.  Characters mapped to None are deleted.\n",
      "     |  \n",
      "     |  upper(self, /)\n",
      "     |      Return a copy of the string converted to uppercase.\n",
      "     |  \n",
      "     |  zfill(self, width, /)\n",
      "     |      Pad a numeric string with zeros on the left, to fill a field of the given width.\n",
      "     |      \n",
      "     |      The string is never truncated.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.str:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  maketrans(...)\n",
      "     |      Return a translation table usable for str.translate().\n",
      "     |      \n",
      "     |      If there is only one argument, it must be a dictionary mapping Unicode\n",
      "     |      ordinals (integers) or characters to Unicode ordinals, strings or None.\n",
      "     |      Character keys will be then converted to ordinals.\n",
      "     |      If there are two arguments, they must be strings of equal length, and\n",
      "     |      in the resulting dictionary, each character in x will be mapped to the\n",
      "     |      character at the same position in y. If there is a third argument, it\n",
      "     |      must be a string, whose characters will be mapped to None in the result.\n",
      "    \n",
      "    class TranslateFolder(tensorflow_datasets.core.dataset_builder.DatasetBuilder)\n",
      "     |  TranslateFolder(root_dir: str)\n",
      "     |  \n",
      "     |  Generic text translation dataset created from manual directory.\n",
      "     |  \n",
      "     |  The directory content should be as followed:\n",
      "     |  \n",
      "     |  ```\n",
      "     |  path/to/my_data/\n",
      "     |    lang1.train.txt\n",
      "     |    lang2.train.txt\n",
      "     |    lang1.test.txt\n",
      "     |    lang2.test.txt\n",
      "     |    ...\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Each files should have one example per line. Line order should match between\n",
      "     |  files.\n",
      "     |  \n",
      "     |  To use it:\n",
      "     |  \n",
      "     |  ```\n",
      "     |  builder = tfds.TranslateFolder(root_dir='path/to/my_data/')\n",
      "     |  print(builder.info)  # Splits, num examples,... are automatically calculated\n",
      "     |  ds = builder.as_dataset(split='train', shuffle_files=True)\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Note: All examples from all splits are loaded in memory in `__init__`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TranslateFolder\n",
      "     |      tensorflow_datasets.core.dataset_builder.DatasetBuilder\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root_dir: str)\n",
      "     |      Constructs a DatasetBuilder.\n",
      "     |      \n",
      "     |      Callers must pass arguments as keyword arguments.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        data_dir: `str`, directory to read/write data. Defaults to the value of\n",
      "     |          the environment variable TFDS_DATA_DIR, if set, otherwise falls back to\n",
      "     |          \"~/tensorflow_datasets\".\n",
      "     |        config: `tfds.core.BuilderConfig` or `str` name, optional configuration\n",
      "     |          for the dataset that affects the data generated on disk. Different\n",
      "     |          `builder_config`s will have their own subdirectories and versions.\n",
      "     |        version: `str`. Optional version at which to load the dataset. An error is\n",
      "     |          raised if specified version cannot be satisfied. Eg: '1.2.3', '1.2.*'.\n",
      "     |          The special value \"experimental_latest\" will use the highest version,\n",
      "     |          even if not default. This is not recommended unless you know what you\n",
      "     |          are doing, as the version could be broken.\n",
      "     |  \n",
      "     |  download_and_prepare(self, **kwargs)\n",
      "     |      Downloads and prepares dataset for reading.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        download_dir: `str`, directory where downloaded files are stored.\n",
      "     |          Defaults to \"~/tensorflow-datasets/downloads\".\n",
      "     |        download_config: `tfds.download.DownloadConfig`, further configuration for\n",
      "     |          downloading and preparing dataset.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        IOError: if there is not enough disk space available.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  VERSION = <tensorflow_datasets.core.utils.version.Version object>\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  name = 'translate_folder'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from tensorflow_datasets.core.dataset_builder.DatasetBuilder:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  as_dataset(self, split=None, batch_size=None, shuffle_files=False, decoders=None, read_config=None, as_supervised=False)\n",
      "     |      Constructs a `tf.data.Dataset`.\n",
      "     |      \n",
      "     |      Callers must pass arguments as keyword arguments.\n",
      "     |      \n",
      "     |      The output types vary depending on the parameters. Examples:\n",
      "     |      \n",
      "     |      ```python\n",
      "     |      builder = tfds.builder('imdb_reviews')\n",
      "     |      builder.download_and_prepare()\n",
      "     |      \n",
      "     |      # Default parameters: Returns the dict of tf.data.Dataset\n",
      "     |      ds_all_dict = builder.as_dataset()\n",
      "     |      assert isinstance(ds_all_dict, dict)\n",
      "     |      print(ds_all_dict.keys())  # ==> ['test', 'train', 'unsupervised']\n",
      "     |      \n",
      "     |      assert isinstance(ds_all_dict['test'], tf.data.Dataset)\n",
      "     |      # Each dataset (test, train, unsup.) consists of dictionaries\n",
      "     |      # {'label': <tf.Tensor: .. dtype=int64, numpy=1>,\n",
      "     |      #  'text': <tf.Tensor: .. dtype=string, numpy=b\"I've watched the movie ..\">}\n",
      "     |      # {'label': <tf.Tensor: .. dtype=int64, numpy=1>,\n",
      "     |      #  'text': <tf.Tensor: .. dtype=string, numpy=b'If you love Japanese ..'>}\n",
      "     |      \n",
      "     |      # With as_supervised: tf.data.Dataset only contains (feature, label) tuples\n",
      "     |      ds_all_supervised = builder.as_dataset(as_supervised=True)\n",
      "     |      assert isinstance(ds_all_supervised, dict)\n",
      "     |      print(ds_all_supervised.keys())  # ==> ['test', 'train', 'unsupervised']\n",
      "     |      \n",
      "     |      assert isinstance(ds_all_supervised['test'], tf.data.Dataset)\n",
      "     |      # Each dataset (test, train, unsup.) consists of tuples (text, label)\n",
      "     |      # (<tf.Tensor: ... dtype=string, numpy=b\"I've watched the movie ..\">,\n",
      "     |      #  <tf.Tensor: ... dtype=int64, numpy=1>)\n",
      "     |      # (<tf.Tensor: ... dtype=string, numpy=b\"If you love Japanese ..\">,\n",
      "     |      #  <tf.Tensor: ... dtype=int64, numpy=1>)\n",
      "     |      \n",
      "     |      # Same as above plus requesting a particular split\n",
      "     |      ds_test_supervised = builder.as_dataset(as_supervised=True, split='test')\n",
      "     |      assert isinstance(ds_test_supervised, tf.data.Dataset)\n",
      "     |      # The dataset consists of tuples (text, label)\n",
      "     |      # (<tf.Tensor: ... dtype=string, numpy=b\"I've watched the movie ..\">,\n",
      "     |      #  <tf.Tensor: ... dtype=int64, numpy=1>)\n",
      "     |      # (<tf.Tensor: ... dtype=string, numpy=b\"If you love Japanese ..\">,\n",
      "     |      #  <tf.Tensor: ... dtype=int64, numpy=1>)\n",
      "     |      ```\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        split: Which split of the data to load (e.g. `'train'`, `'test'`\n",
      "     |          `['train', 'test']`, `'train[80%:]'`,...). See our\n",
      "     |          [split API guide](https://www.tensorflow.org/datasets/splits).\n",
      "     |          If `None`, will return all splits in a `Dict[Split, tf.data.Dataset]`.\n",
      "     |        batch_size: `int`, batch size. Note that variable-length features will\n",
      "     |          be 0-padded if `batch_size` is set. Users that want more custom behavior\n",
      "     |          should use `batch_size=None` and use the `tf.data` API to construct a\n",
      "     |          custom pipeline. If `batch_size == -1`, will return feature\n",
      "     |          dictionaries of the whole dataset with `tf.Tensor`s instead of a\n",
      "     |          `tf.data.Dataset`.\n",
      "     |        shuffle_files: `bool`, whether to shuffle the input files. Defaults to\n",
      "     |          `False`.\n",
      "     |        decoders: Nested dict of `Decoder` objects which allow to customize the\n",
      "     |          decoding. The structure should match the feature structure, but only\n",
      "     |          customized feature keys need to be present. See\n",
      "     |          [the guide](https://github.com/tensorflow/datasets/tree/master/docs/decode.md)\n",
      "     |          for more info.\n",
      "     |        read_config: `tfds.ReadConfig`, Additional options to configure the\n",
      "     |          input pipeline (e.g. seed, num parallel reads,...).\n",
      "     |        as_supervised: `bool`, if `True`, the returned `tf.data.Dataset`\n",
      "     |          will have a 2-tuple structure `(input, label)` according to\n",
      "     |          `builder.info.supervised_keys`. If `False`, the default,\n",
      "     |          the returned `tf.data.Dataset` will have a dictionary with all the\n",
      "     |          features.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        `tf.data.Dataset`, or if `split=None`, `dict<key: tfds.Split, value:\n",
      "     |        tfds.data.Dataset>`.\n",
      "     |      \n",
      "     |        If `batch_size` is -1, will return feature dictionaries containing\n",
      "     |        the entire dataset in `tf.Tensor`s instead of a `tf.data.Dataset`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from tensorflow_datasets.core.dataset_builder.DatasetBuilder:\n",
      "     |  \n",
      "     |  builder_config\n",
      "     |      `tfds.core.BuilderConfig` for this builder.\n",
      "     |  \n",
      "     |  builder_configs\n",
      "     |      classmethod(function) -> method\n",
      "     |      \n",
      "     |      Convert a function to be a class method.\n",
      "     |      \n",
      "     |      A class method receives the class as implicit first argument,\n",
      "     |      just like an instance method receives the instance.\n",
      "     |      To declare a class method, use this idiom:\n",
      "     |      \n",
      "     |        class C:\n",
      "     |            @classmethod\n",
      "     |            def f(cls, arg1, arg2, ...):\n",
      "     |                ...\n",
      "     |      \n",
      "     |      It can be called either on the class (e.g. C.f()) or on an instance\n",
      "     |      (e.g. C().f()).  The instance is ignored except for its class.\n",
      "     |      If a class method is called for a derived class, the derived class\n",
      "     |      object is passed as the implied first argument.\n",
      "     |      \n",
      "     |      Class methods are different than C++ or Java static methods.\n",
      "     |      If you want those, see the staticmethod builtin.\n",
      "     |  \n",
      "     |  canonical_version\n",
      "     |  \n",
      "     |  data_dir\n",
      "     |  \n",
      "     |  info\n",
      "     |      `tfds.core.DatasetInfo` for this builder.\n",
      "     |  \n",
      "     |  supported_versions\n",
      "     |  \n",
      "     |  version\n",
      "     |  \n",
      "     |  versions\n",
      "     |      Versions (canonical + availables), in preference order.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from tensorflow_datasets.core.dataset_builder.DatasetBuilder:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from tensorflow_datasets.core.dataset_builder.DatasetBuilder:\n",
      "     |  \n",
      "     |  BUILDER_CONFIGS = []\n",
      "     |  \n",
      "     |  IN_DEVELOPMENT = False\n",
      "     |  \n",
      "     |  MANUAL_DOWNLOAD_INSTRUCTIONS = None\n",
      "     |  \n",
      "     |  SUPPORTED_VERSIONS = []\n",
      "\n",
      "FUNCTIONS\n",
      "    as_numpy(dataset, graph=None)\n",
      "        Converts a `tf.data.Dataset` to an iterable of NumPy arrays.\n",
      "        \n",
      "        `as_numpy` converts a possibly nested structure of `tf.data.Dataset`s\n",
      "        and `tf.Tensor`s to iterables of NumPy arrays and NumPy arrays, respectively.\n",
      "        \n",
      "        Note that because TensorFlow has support for ragged tensors and NumPy has\n",
      "        no equivalent representation,\n",
      "        [`tf.RaggedTensor`s](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor)\n",
      "        are left as-is for the user to deal with them (e.g. using `to_list()`).\n",
      "        In TF 1 (i.e. graph mode), `tf.RaggedTensor`s are returned as\n",
      "        `tf.ragged.RaggedTensorValue`s.\n",
      "        \n",
      "        Example:\n",
      "        \n",
      "        ```\n",
      "        ds = tfds.load(name=\"mnist\", split=\"train\")\n",
      "        ds_numpy = tfds.as_numpy(ds)  # Convert `tf.data.Dataset` to Python generator\n",
      "        for ex in ds_numpy:\n",
      "          # `{'image': np.array(shape=(28, 28, 1)), 'labels': np.array(shape=())}`\n",
      "          print(ex)\n",
      "        ```\n",
      "        \n",
      "        Args:\n",
      "          dataset: a possibly nested structure of `tf.data.Dataset`s and/or\n",
      "            `tf.Tensor`s.\n",
      "          graph: `tf.Graph`, optional, explicitly set the graph to use.\n",
      "        \n",
      "        Returns:\n",
      "          A structure matching `dataset` where `tf.data.Dataset`s are converted to\n",
      "          generators of NumPy arrays and `tf.Tensor`s are converted to NumPy arrays.\n",
      "    \n",
      "    builder(name, **builder_init_kwargs)\n",
      "        Fetches a `tfds.core.DatasetBuilder` by string name.\n",
      "        \n",
      "        Args:\n",
      "          name: `str`, the registered name of the `DatasetBuilder` (the class name\n",
      "            as camel or snake case: `MyDataset` or `my_dataset`).\n",
      "            This can be either `'dataset_name'` or\n",
      "            `'dataset_name/config_name'` for datasets with `BuilderConfig`s.\n",
      "            As a convenience, this string may contain comma-separated keyword\n",
      "            arguments for the builder. For example `'foo_bar/a=True,b=3'` would use\n",
      "            the `FooBar` dataset passing the keyword arguments `a=True` and `b=3`\n",
      "            (for builders with configs, it would be `'foo_bar/zoo/a=True,b=3'` to\n",
      "            use the `'zoo'` config and pass to the builder keyword arguments `a=True`\n",
      "            and `b=3`).\n",
      "          **builder_init_kwargs: `dict` of keyword arguments passed to the\n",
      "            `DatasetBuilder`. These will override keyword arguments passed in `name`,\n",
      "            if any.\n",
      "        \n",
      "        Returns:\n",
      "          A `tfds.core.DatasetBuilder`.\n",
      "        \n",
      "        Raises:\n",
      "          DatasetNotFoundError: if `name` is unrecognized.\n",
      "    \n",
      "    builder_cls(name: str)\n",
      "        Fetches a `tfds.core.DatasetBuilder` class by string name.\n",
      "        \n",
      "        Args:\n",
      "          name: `str`, the registered name of the `DatasetBuilder` (the class name\n",
      "            as camel or snake case: `MyDataset` or `my_dataset`).\n",
      "        \n",
      "        Returns:\n",
      "          A `tfds.core.DatasetBuilder` class.\n",
      "        \n",
      "        Raises:\n",
      "          DatasetNotFoundError: if `name` is unrecognized.\n",
      "    \n",
      "    disable_progress_bar()\n",
      "        Disabled Tqdm progress bar.\n",
      "        \n",
      "        Usage:\n",
      "        \n",
      "        tfds.disable_progress_bar()\n",
      "    \n",
      "    is_dataset_on_gcs(dataset_name: str) -> bool\n",
      "        If the dataset is available on the GCS bucket gs://tfds-data/datasets.\n",
      "    \n",
      "    list_builders()\n",
      "        Returns the string names of all `tfds.core.DatasetBuilder`s.\n",
      "    \n",
      "    load(name, split=None, data_dir=None, batch_size=None, shuffle_files=False, download=True, as_supervised=False, decoders=None, read_config=None, with_info=False, builder_kwargs=None, download_and_prepare_kwargs=None, as_dataset_kwargs=None, try_gcs=False)\n",
      "        Loads the named dataset into a `tf.data.Dataset`.\n",
      "        \n",
      "        If `split=None` (the default), returns all splits for the dataset. Otherwise,\n",
      "        returns the specified split.\n",
      "        \n",
      "        `load` is a convenience method that fetches the `tfds.core.DatasetBuilder` by\n",
      "        string name, optionally calls `DatasetBuilder.download_and_prepare`\n",
      "        (if `download=True`), and then calls `DatasetBuilder.as_dataset`.\n",
      "        This is roughly equivalent to:\n",
      "        \n",
      "        ```\n",
      "        builder = tfds.builder(name, data_dir=data_dir, **builder_kwargs)\n",
      "        if download:\n",
      "          builder.download_and_prepare(**download_and_prepare_kwargs)\n",
      "        ds = builder.as_dataset(\n",
      "            split=split, as_supervised=as_supervised, **as_dataset_kwargs)\n",
      "        if with_info:\n",
      "          return ds, builder.info\n",
      "        return ds\n",
      "        ```\n",
      "        \n",
      "        If you'd like NumPy arrays instead of `tf.data.Dataset`s or `tf.Tensor`s,\n",
      "        you can pass the return value to `tfds.as_numpy`.\n",
      "        \n",
      "        Callers must pass arguments as keyword arguments.\n",
      "        \n",
      "        **Warning**: calling this function might potentially trigger the download\n",
      "        of hundreds of GiB to disk. Refer to the `download` argument.\n",
      "        \n",
      "        Args:\n",
      "          name: `str`, the registered name of the `DatasetBuilder` (the snake case\n",
      "            version of the class name). This can be either `\"dataset_name\"` or\n",
      "            `\"dataset_name/config_name\"` for datasets with `BuilderConfig`s.\n",
      "            As a convenience, this string may contain comma-separated keyword\n",
      "            arguments for the builder. For example `\"foo_bar/a=True,b=3\"` would use\n",
      "            the `FooBar` dataset passing the keyword arguments `a=True` and `b=3`\n",
      "            (for builders with configs, it would be `\"foo_bar/zoo/a=True,b=3\"` to\n",
      "            use the `\"zoo\"` config and pass to the builder keyword arguments `a=True`\n",
      "            and `b=3`).\n",
      "          split: Which split of the data to load (e.g. `'train'`, `'test'`\n",
      "            `['train', 'test']`, `'train[80%:]'`,...). See our\n",
      "            [split API guide](https://www.tensorflow.org/datasets/splits).\n",
      "            If `None`, will return all splits in a `Dict[Split, tf.data.Dataset]`\n",
      "          data_dir: `str`, directory to read/write data. Defaults to the value of\n",
      "            the environment variable TFDS_DATA_DIR, if set, otherwise falls back to\n",
      "            \"~/tensorflow_datasets\".\n",
      "          batch_size: `int`, if set, add a batch dimension to examples. Note that\n",
      "            variable length features will be 0-padded. If\n",
      "            `batch_size=-1`, will return the full dataset as `tf.Tensor`s.\n",
      "          shuffle_files: `bool`, whether to shuffle the input files.\n",
      "            Defaults to `False`.\n",
      "          download: `bool` (optional), whether to call\n",
      "            `tfds.core.DatasetBuilder.download_and_prepare`\n",
      "            before calling `tf.DatasetBuilder.as_dataset`. If `False`, data is\n",
      "            expected to be in `data_dir`. If `True` and the data is already in\n",
      "            `data_dir`, `download_and_prepare` is a no-op.\n",
      "          as_supervised: `bool`, if `True`, the returned `tf.data.Dataset`\n",
      "            will have a 2-tuple structure `(input, label)` according to\n",
      "            `builder.info.supervised_keys`. If `False`, the default,\n",
      "            the returned `tf.data.Dataset` will have a dictionary with all the\n",
      "            features.\n",
      "          decoders: Nested dict of `Decoder` objects which allow to customize the\n",
      "            decoding. The structure should match the feature structure, but only\n",
      "            customized feature keys need to be present. See\n",
      "            [the guide](https://github.com/tensorflow/datasets/tree/master/docs/decode.md)\n",
      "            for more info.\n",
      "          read_config: `tfds.ReadConfig`, Additional options to configure the\n",
      "            input pipeline (e.g. seed, num parallel reads,...).\n",
      "          with_info: `bool`, if True, tfds.load will return the tuple\n",
      "            (tf.data.Dataset, tfds.core.DatasetInfo) containing the info associated\n",
      "            with the builder.\n",
      "          builder_kwargs: `dict` (optional), keyword arguments to be passed to the\n",
      "            `tfds.core.DatasetBuilder` constructor. `data_dir` will be passed\n",
      "            through by default.\n",
      "          download_and_prepare_kwargs: `dict` (optional) keyword arguments passed to\n",
      "            `tfds.core.DatasetBuilder.download_and_prepare` if `download=True`. Allow\n",
      "            to control where to download and extract the cached data. If not set,\n",
      "            cache_dir and manual_dir will automatically be deduced from data_dir.\n",
      "          as_dataset_kwargs: `dict` (optional), keyword arguments passed to\n",
      "            `tfds.core.DatasetBuilder.as_dataset`.\n",
      "          try_gcs: `bool`, if True, tfds.load will see if the dataset exists on\n",
      "            the public GCS bucket before building it locally.\n",
      "        \n",
      "        Returns:\n",
      "          ds: `tf.data.Dataset`, the dataset requested, or if `split` is None, a\n",
      "            `dict<key: tfds.Split, value: tfds.data.Dataset>`. If `batch_size=-1`,\n",
      "            these will be full datasets as `tf.Tensor`s.\n",
      "          ds_info: `tfds.core.DatasetInfo`, if `with_info` is True, then `tfds.load`\n",
      "            will return a tuple `(ds, ds_info)` containing dataset information\n",
      "            (version, features, splits, num_examples,...). Note that the `ds_info`\n",
      "            object documents the entire dataset, regardless of the `split` requested.\n",
      "            Split-specific information is available in `ds_info.splits`.\n",
      "    \n",
      "    show_examples(ds: tensorflow.python.data.ops.dataset_ops.DatasetV2, ds_info: tensorflow_datasets.core.dataset_info.DatasetInfo, **options_kwargs: Any)\n",
      "        Visualize images (and labels) from an image classification dataset.\n",
      "        \n",
      "        This function is for interactive use (Colab, Jupyter). It displays and return\n",
      "        a plot of (rows*columns) images from a tf.data.Dataset.\n",
      "        \n",
      "        Usage:\n",
      "        ```python\n",
      "        ds, ds_info = tfds.load('cifar10', split='train', with_info=True)\n",
      "        fig = tfds.show_examples(ds, ds_info)\n",
      "        ```\n",
      "        \n",
      "        Args:\n",
      "          ds: `tf.data.Dataset`. The tf.data.Dataset object to visualize. Examples\n",
      "            should not be batched. Examples will be consumed in order until\n",
      "            (rows * cols) are read or the dataset is consumed.\n",
      "          ds_info: The dataset info object to which extract the label and features\n",
      "            info. Available either through `tfds.load('mnist', with_info=True)` or\n",
      "            `tfds.builder('mnist').info`\n",
      "          **options_kwargs: Additional display options, specific to the dataset type\n",
      "            to visualize. Are forwarded to `tfds.visualization.Visualizer.show`.\n",
      "            See the `tfds.visualization` for a list of available visualizers.\n",
      "        \n",
      "        Returns:\n",
      "          fig: The `matplotlib.Figure` object\n",
      "    \n",
      "    show_statistics(ds_info: tensorflow_datasets.core.dataset_info.DatasetInfo, split: tensorflow_datasets.core.splits.Split = Split('train'), disable_logging: bool = True) -> None\n",
      "        Display the datasets statistics on a Colab/Jupyter notebook.\n",
      "        \n",
      "        `tfds.show_statistics` is a wrapper around\n",
      "        [tensorflow_data_validation](https://www.tensorflow.org/tfx/data_validation/get_started)\n",
      "        which calls `tfdv.visualize_statistics`. Statistics are displayed using\n",
      "        [FACETS OVERVIEW](https://pair-code.github.io/facets/).\n",
      "        \n",
      "        Usage:\n",
      "        \n",
      "        ```\n",
      "        builder = tfds.builder('mnist')\n",
      "        tfds.show_statistics(builder.info)\n",
      "        ```\n",
      "        \n",
      "        Or:\n",
      "        \n",
      "        ```\n",
      "        ds, ds_info = tfds.load('mnist', with_info)\n",
      "        tfds.show_statistics(ds_info)\n",
      "        ```\n",
      "        \n",
      "        Note: In order to work, `tensorflow_data_validation` must be installed and\n",
      "        the dataset info object must contain the statistics. For \"official\" datasets,\n",
      "        only datasets which have been added/updated recently will contains statistics.\n",
      "        For \"custom\" datasets, you need to generate the dataset with\n",
      "        `tensorflow_data_validation` installed to have the statistics.\n",
      "        \n",
      "        Args:\n",
      "          ds_info: The `tfds.core.DatasetInfo` object containing the statistics.\n",
      "          split: Split for which generate the statistics.\n",
      "          disable_logging: `bool`, if True, disable the tfdv logs which can be\n",
      "            too verbose.\n",
      "        \n",
      "        Returns:\n",
      "          `None`\n",
      "\n",
      "DATA\n",
      "    __all__ = ['as_numpy', 'core', 'builder', 'builder_cls', 'decode', 'di...\n",
      "\n",
      "VERSION\n",
      "    3.2.1\n",
      "\n",
      "FILE\n",
      "    /home/drishtant/anaconda3/lib/python3.8/site-packages/tensorflow_datasets/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tfds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data , test_data) , info_data = tfds.load('emnist',\n",
    "                                                split=[\"train[:85%]\", \"train[85%:]\"],\n",
    "                                                as_supervised=True,\n",
    "                                                with_info = True,\n",
    "                                                shuffle_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='emnist',\n",
      "    version=3.0.0,\n",
      "    description='The EMNIST dataset is a set of handwritten character digits derived from the NIST Special Database 19 and converted to a 28x28 pixel image format and dataset structure that directly matches the MNIST dataset.\n",
      "\n",
      "Note: Like the original EMNIST data, images provided here are inverted horizontally and rotated 90 anti-clockwise. You can use `tf.transpose` within `ds.map` to convert the images to a human-friendlier format.',\n",
      "    homepage='https://www.nist.gov/itl/products-and-services/emnist-dataset',\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=62),\n",
      "    }),\n",
      "    total_num_examples=814255,\n",
      "    splits={\n",
      "        'test': 116323,\n",
      "        'train': 697932,\n",
      "    },\n",
      "    supervised_keys=('image', 'label'),\n",
      "    citation=\"\"\"@article{cohen_afshar_tapson_schaik_2017,\n",
      "        title={EMNIST: Extending MNIST to handwritten letters},\n",
      "        DOI={10.1109/ijcnn.2017.7966217},\n",
      "        journal={2017 International Joint Conference on Neural Networks (IJCNN)},\n",
      "        author={Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and Schaik, Andre Van},\n",
      "        year={2017}\n",
      "    }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(info_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_and_flatten (image,label):\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32) # scale image pixels to [0,1]\n",
    "    image = tf.transpose(image, [1,0,2]) # transpose to get human friendly image, since rotation\n",
    "    image = tf.reshape(image, shape=(784,)) # permutation invariant or flatten \n",
    "    label = tf.one_hot(label, depth=62) # one hot encode label\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_data.shuffle(1024)#this will shuffle dataset to avoid the machine to learn the sequence of data.\n",
    "train_set = train_set.map(transpose_and_flatten, num_parallel_calls = tf.data.experimental.AUTOTUNE)# here we are \n",
    "                            # mapping the function to process to whole rows of the dataset.\n",
    "train_set = train_set.batch(128)# now this step will form dataset in Batches of 128 units each.\n",
    "train_set = train_set.prefetch(tf.data.experimental.AUTOTUNE)# this will enable the fetching of data before sending \n",
    "                            # it into neural network pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = test_data.shuffle(1024)\n",
    "test_set = test_set.map(transpose_and_flatten, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "test_set = test_set.batch(128)\n",
    "test_set = test_set.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense , Dropout \n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units = 128, activation = \"relu\", input_dim = 784))\n",
    "model.add(Dense(units = 128, activation = \"relu\"))\n",
    "model.add(Dense(units = 96 , activation = \"relu\"))\n",
    "model.add(Dense(units = 78 , activation = \"relu\"))\n",
    "\n",
    "model.add(Dense(units = 64 , activation = \"softmax\"))\n",
    "\n",
    "model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
